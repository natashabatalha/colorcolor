{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning!!! Avert your eyes and proceed to notebooks folder. \n",
    "\n",
    "This notebook is just for the developer's reference. It is a stream of thought multivariate analysis so I could design the stats module. Wanted to add it to scripts so that I could reference it if need be. But it is not recommended to use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "import os \n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import *\n",
    "output_notebook()\n",
    "import pickle as pk\n",
    "from itertools import combinations as comb\n",
    "\n",
    "from pydoc import help  # can type in the python console `help(name of function)` to get the documentation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import colcol as c\n",
    "# figures inline in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DISPLAY_MAX_ROWS = 20  # number of max rows to print for a DataFrame\n",
    "pd.set_option('display.max_rows', DISPLAY_MAX_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = c.print_filters('wfirst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirr = os.getenv('ALBEDO_DB')\n",
    "engine = create_engine('sqlite:///'+dirr+'AlbedoModels_2015.db')\n",
    "header = pd.read_sql_table('header',engine)\n",
    "et= pk.load(open('/Users/natashabatalha/Documents/colorcolor/notebooks/wfirst_colors_dataframe.pk','rb'))#pk.load(open('/Volumes/MyPassportforMac/WFIRST/FluxDataFrameWFIRST.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et = et.reset_index(drop=True)\n",
    "everything=et.dropna()[~et.dropna().isin([np.inf, -np.inf])]\n",
    "everything = everything.dropna()[(everything['cloud']==0) & (everything['phase']==90)]\n",
    "yofinterest = 'metallicity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#everything\n",
    "fcomb = [i[0]+i[1] for i in comb(fs,2)] +list(fs)#independent varianbles \n",
    "#fcomb = [i for i in fs]\n",
    "#fcomb += [ '506', '575', '661', '721', '883', '940']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corrmat = everything.loc[:,fcomb].corr()\n",
    "corrmat = everything.loc[(everything['cloud']==0)].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,10)) \n",
    "sns.heatmap(corrmat, vmax=1, square=False, linewidths=.5, ax=ax).xaxis.tick_top()\n",
    "#.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mosthighlycorrelated(mydataframe, numtoreport):\n",
    "    # find the correlations\n",
    "    cormatrix = mydataframe.corr()\n",
    "    # set the correlations on the diagonal or lower triangle to zero,\n",
    "    # so they will not be reported as the highest ones:\n",
    "    cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n",
    "    # find the top n correlations\n",
    "    cormatrix = cormatrix.stack()\n",
    "    cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n",
    "    # assign human-friendly names\n",
    "    cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n",
    "    return cormatrix.head(numtoreport)\n",
    "x = mosthighlycorrelated(everything.loc[:,fcomb].corr(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardisedX = scale(everything.loc[:,fcomb])\n",
    "standardisedX = pd.DataFrame(standardisedX, index=everything.loc[:,fcomb].index, columns=everything.loc[:,fcomb].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(standardisedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_summary(pca, standardised_data, out=True):\n",
    "    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    \n",
    "    a = list(np.std(pca.transform(standardised_data), axis=0))\n",
    "    b = list(pca.explained_variance_ratio_)\n",
    "    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n",
    "    summary = pd.DataFrame(np.c_[a, b, c], index=names, columns=columns)\n",
    "    if out:\n",
    "        print(\"Importance of components:\")\n",
    "        display(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pca_summary(pca, standardisedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(summary.sdev**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screeplot(pca, standardised_values):\n",
    "    y = np.std(pca.transform(standardised_values), axis=0)**2\n",
    "    x = np.arange(len(y)) + 1\n",
    "    plt.plot(x, y, \"o-\")\n",
    "    plt.xticks(x, [\"Comp.\"+str(i) for i in x], rotation=60)\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.show()\n",
    "\n",
    "screeplot(pca, standardisedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sdev**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(pca.components_[0], fcomb): print( i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcpc(variables, loadings):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # make a vector to store the component\n",
    "    pc = np.zeros(numsamples)\n",
    "    # calculate the value of the component for each sample\n",
    "    for i in range(numsamples):\n",
    "        valuei = 0\n",
    "        for j in range(numvariables):\n",
    "            valueij = variables.iloc[i, j]\n",
    "            loadingj = loadings[j]\n",
    "            valuei = valuei + (valueij * loadingj)\n",
    "        pc[i] = valuei\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcpc(standardisedX, pca.components_[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_scatter(pca, standardised_values, classifs):\n",
    "    foo = pca.transform(standardised_values)\n",
    "    bar = pd.DataFrame(list(zip(foo[:, 0], foo[:, 1], classifs)), columns=[\"PC1\", \"PC2\", \"Class\"])\n",
    "    sns.lmplot(\"PC1\", \"PC2\", bar, hue=\"Class\", fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = everything.loc[:,fcomb]\n",
    "y =  everything[yofinterest].astype(str)\n",
    "#newy = []\n",
    "#for i in y: \n",
    "#    if float(i) >= 1.7: newy+=['3']\n",
    "#    elif float(i) <= 0.5: newy+=['1']\n",
    "#    else: newy +=['2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scatter(pca, standardisedX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classifier so needs to be string.. \n",
    "lda = LinearDiscriminantAnalysis().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_scalings(lda, X, out=False):\n",
    "    ret = pd.DataFrame(lda.scalings_, index=X.columns, columns=[\"LD\"+str(i+1) for i in range(lda.scalings_.shape[1])])\n",
    "    if out:\n",
    "        print(\"Coefficients of linear discriminants:\")\n",
    "        display(ret)\n",
    "    return ret\n",
    "\n",
    "pretty_scalings_ = pretty_scalings(lda, X, out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_scalings_.LD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcWithinGroupsVariance(variable, groupvariable):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set(groupvariable))\n",
    "    numlevels = len(levels)\n",
    "    # get the mean and standard deviation for each group:\n",
    "    numtotal = 0\n",
    "    denomtotal = 0\n",
    "    for leveli in levels:\n",
    "        levelidata = variable[groupvariable==leveli]\n",
    "        levelilength = len(levelidata)\n",
    "        # get the standard deviation for group i:\n",
    "        sdi = np.std(levelidata)\n",
    "        numi = (levelilength)*sdi**2\n",
    "        denomi = levelilength\n",
    "        numtotal = numtotal + numi\n",
    "        denomtotal = denomtotal + denomi\n",
    "    # calculate the within-groups variance\n",
    "    Vw = numtotal / (denomtotal - numlevels)\n",
    "    return Vw\n",
    "def calcBetweenGroupsVariance(variable, groupvariable):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set((groupvariable)))\n",
    "    numlevels = len(levels)\n",
    "    # calculate the overall grand mean:\n",
    "    grandmean = np.mean(variable)\n",
    "    # get the mean and standard deviation for each group:\n",
    "    numtotal = 0\n",
    "    denomtotal = 0\n",
    "    for leveli in levels:\n",
    "        levelidata = variable[groupvariable==leveli]\n",
    "        levelilength = len(levelidata)\n",
    "        # get the mean and standard deviation for group i:\n",
    "        meani = np.mean(levelidata)\n",
    "        sdi = np.std(levelidata)\n",
    "        numi = levelilength * ((meani - grandmean)**2)\n",
    "        denomi = levelilength\n",
    "        numtotal = numtotal + numi\n",
    "        denomtotal = denomtotal + denomi\n",
    "    # calculate the between-groups variance\n",
    "    Vb = numtotal / (numlevels - 1)\n",
    "    return(Vb)\n",
    "def calclda(variables, loadings):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # make a vector to store the discriminant function\n",
    "    ld = np.zeros(numsamples)\n",
    "    # calculate the value of the discriminant function for each sample\n",
    "    for i in range(numsamples):\n",
    "        valuei = 0\n",
    "        for j in range(numvariables):\n",
    "            valueij = variables.iloc[i, j]\n",
    "            loadingj = loadings[j]\n",
    "            valuei = valuei + (valueij * loadingj)\n",
    "        ld[i] = valuei\n",
    "    # standardise the discriminant function so that its mean value is 0:\n",
    "    ld = scale(ld, with_std=False)\n",
    "    return ld\n",
    "def groupStandardise(variables, groupvariable):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # find the variable names\n",
    "    variablenames = variables.columns\n",
    "    # calculate the group-standardised version of each variable\n",
    "    variables_new = pd.DataFrame()\n",
    "    for i in range(numvariables):\n",
    "        variable_name = variablenames[i]\n",
    "        variablei = variables[variable_name]\n",
    "        variablei_Vw = calcWithinGroupsVariance(variablei, groupvariable)\n",
    "        variablei_mean = np.mean(variablei)\n",
    "        variablei_new = (variablei - variablei_mean)/(np.sqrt(variablei_Vw))\n",
    "        variables_new[variable_name] = variablei_new\n",
    "    return variables_new\n",
    "def calcSeparations(variables, groupvariable):\n",
    "    # calculate the separation for each variable\n",
    "    for variablename in variables:\n",
    "        variablei = variables[variablename]\n",
    "        Vw = calcWithinGroupsVariance(variablei, groupvariable)\n",
    "        Vb = calcBetweenGroupsVariance(variablei, groupvariable)\n",
    "        sep = Vb/Vw\n",
    "        print(\"variable\", variablename, \"Vw=\", Vw, \"Vb=\", Vb, \"separation=\", sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calclda(X, lda.scalings_[:, 0]))\n",
    "print(lda.fit_transform(X, y)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupstandardisedX = groupStandardise(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2 = LinearDiscriminantAnalysis().fit(groupstandardisedX, y)\n",
    "pretty_scalings(lda2, groupstandardisedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit_transform(X, y)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2.fit_transform(groupstandardisedX, y)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rpredict(lda, X, y, out=False):\n",
    "    ret = {\"class\": lda.predict(X),\n",
    "           \"posterior\": pd.DataFrame(lda.predict_proba(X), columns=lda.classes_)}\n",
    "    ret[\"x\"] = pd.DataFrame(lda.fit_transform(X, y))\n",
    "    ret[\"x\"].columns = [\"LD\"+str(i+1) for i in range(ret[\"x\"].shape[1])]\n",
    "    #if out:\n",
    "    #    print(\"class\")\n",
    "    #    print(ret['class'])# : print(i)\n",
    "    #    print()\n",
    "    #    print(\"posterior\")\n",
    "    #    print(ret[\"posterior\"])\n",
    "    #    print()\n",
    "    #    print(\"x\")\n",
    "    #    print(ret[\"x\"])\n",
    "    return ret\n",
    "\n",
    "lda_values = rpredict(lda2, standardisedX.reset_index(drop=True), y.reset_index(drop=True), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcSeparations(lda_values[\"x\"], y.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ldahist(data, g, sep=False):\n",
    "    xmin = np.trunc(np.min(data)) - 1\n",
    "    xmax = np.trunc(np.max(data)) + 1\n",
    "    ncol = len(set(g))\n",
    "    print (ncol)\n",
    "    binwidth = 0.5\n",
    "    bins=np.arange(xmin, xmax + binwidth, binwidth)\n",
    "    if sep:\n",
    "        fig, axl = plt.subplots(ncol, 1, sharey=True, sharex=True)\n",
    "    else:\n",
    "        fig, axl = plt.subplots(1, 1, sharey=True, sharex=True)\n",
    "        axl = [axl]*ncol\n",
    "    for ax, (group, gdata) in zip(axl, data.groupby(g)):\n",
    "        sns.distplot(gdata.values, bins, ax=ax, label=\"group \"+str(group))\n",
    "        ax.set_xlim([xmin, xmax])\n",
    "        if sep:\n",
    "            ax.set_xlabel(\"group\"+str(group))\n",
    "        else:\n",
    "            ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldahist(lda_values[\"x\"].LD1, y.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldahist(lda_values[\"x\"].LD2, y.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"LD1\", \"LD2\", lda_values[\"x\"].join(y.reset_index(drop=True)), hue=yofinterest, fit_reg=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMeanAndSdByGroup(variables, groupvariable):\n",
    "    data_groupby = variables.groupby(groupvariable)\n",
    "    print(\"## Means:\")\n",
    "    display(data_groupby.apply(np.mean))\n",
    "    print(\"\\n## Standard deviations:\")\n",
    "    display(data_groupby.apply(np.std))\n",
    "    print(\"\\n## Sample sizes:\")\n",
    "    display(pd.DataFrame(data_groupby.apply(len)))\n",
    "    return data_groupby.apply(np.mean)\n",
    "means = printMeanAndSdByGroup(lda_values[\"x\"], y.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = []\n",
    "for i in range(len(means['LD1'])): \n",
    "    cutoffs +=[np.mean(means['LD1'][i:i+2])]\n",
    "cutoffs = cutoffs[:-1]\n",
    "print(cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_classify(v, levels, cutoffpoints):\n",
    "    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n",
    "        if v > cutoff: return level\n",
    "    return levels[0]\n",
    "    \n",
    "y_pred = lda_values[\"x\"].iloc[:, 0].apply(lda_classify, args=(lda.classes_, cutoffs,)).values\n",
    "y_true = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webprint_confusion_matrix(confusion_matrix, classes_names):\n",
    "    display(pd.DataFrame(confusion_matrix, index=[\"Is group \"+i for i in classes_names], columns=[\"Allocated to group \"+i for i in classes_names]))\n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "webprint_confusion_matrix(cm, lda.classes_)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm_normalized, lda.classes_, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything.loc[:,fcomb].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
